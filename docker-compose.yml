version: '3.8'

services:
  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: blog-generator-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0

    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command: 
      - |
        echo "Starting Ollama server..."
        ollama serve &
        OLLAMA_PID=$!
        
        echo "Waiting for Ollama to be ready..."
        sleep 10
        
        echo "Pulling llama3.2:3b model..."
        ollama pull llama3.2:3b
        
        echo "Model ready! Ollama is now serving on port 11434"
        wait $OLLAMA_PID

  # Blog Generator API
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PYTHON_VERSION: "3.13.3"
    container_name: blog-generator-api
    ports:
      - "8080:8080"

    depends_on:
      ollama:
        condition: service_started 
    environment:
      # Ollama connection
      - OLLAMA_HOST=http://ollama:11434
      - MODEL_NAME=${MODEL_NAME:-llama3.2:3b}
      
      # Document configuration
      - EXAMPLE_POST_FILE=${EXAMPLE_POST_FILE:-example_post.pdf}
      - COMPANY_DESCRIPTION_FILE=${COMPANY_DESCRIPTION_FILE:-company_description.pdf}
      - DOCUMENTS_DIR=${DOCUMENTS_DIR:-documents}
      
      # Default settings
      - DEFAULT_TONE=${DEFAULT_TONE:-professional}
      - DEFAULT_TEMPERATURE=${DEFAULT_TEMPERATURE:-0.7}
      
      # Validation settings
      - MAX_PURPOSE_LENGTH=${MAX_PURPOSE_LENGTH:-500}
      
      # Python settings
      - PYTHONUNBUFFERED=1
    volumes:
      # Mount your documents folder (read-only)
      - ./documents:/app/documents:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

volumes:
  ollama_data:
    driver: local
