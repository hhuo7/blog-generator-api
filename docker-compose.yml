version: '3.8'

services:
  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: blog-generator-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      # Exposes Ollama to the Docker network properly
      - OLLAMA_HOST=0.0.0.0
      
    # Use the standard ollama serve command
    command: ["serve"]
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      # Increased start_period to give the server ample time to start before the API relies on it
      start_period: 300s 
    restart: unless-stopped

  # Blog Generator API
  api:
    build:
      context: .
      # NOTE: Ensure your Dockerfile's FROM line matches a stable version (e.g., 3.12-slim)
      dockerfile: Dockerfile
      # Removed unused 'args' for PYTHON_VERSION unless it's strictly required by the Dockerfile
    container_name: blog-generator-api
    ports:
      - "8080:8080"
      
    # Command to pull the model *before* starting Uvicorn
    # This ensures the model is present and ready on the first run.
    command: >
      /bin/sh -c "
      echo 'Attempting to pull model from API service...' &&
      curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"llama3.2:3b\"}' || true &&
      echo 'Starting Uvicorn...' &&
      exec uvicorn main:app --host 0.0.0.0 --port 8080
      "

    depends_on:
      # Only requires the Ollama server process to be running,
      # as the model pull is now handled by the API's command block
      ollama:
        condition: service_started 
        
    environment:
      # API uses the internal service name 'ollama'
      OLLAMA_HOST: http://ollama:11434
      PYTHONUNBUFFERED: 1
    volumes:
      - ./documents:/app/documents:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

volumes:
  ollama_data:
    driver: local